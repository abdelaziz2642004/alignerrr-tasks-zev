Code Human Preference Tips
Updated December 11th, 2025
The purpose of this document is to provide some tips and clarifications to help boost the
quality of your work contributing to the Code Human Preference Prod project. Thank you to
everyone for your hard work on this project so far!

Important Reminders
Please ensure conversations are always 2+ turns in length. This means at least 2
user prompts, 2 model responses, and 2 ratings + comments.
Come up with original prompts. We see a lot of "add a CLI flag" style prompts that
were essentially seeded or too closely inspired from the examples provided in the
official project instructions.
Make sure to write thorough and detailed comments with your ratings. This is not
aptional.
- Please refrain from prompting the LLM adversarially. Prompts should always reflect
genuine requests a contributor to the codebase would actually make.
Al use of any kind in this project is strictly prohibited apart from using it mildly to
introspect and understand a new codebase if that saves you time.

On another note, we also noticed several submissions ended prematurely while there were
still obvious issues in the final solution worth steering the model to self-correct. Please
take care to ensure conversations do not end while any of the following issues are still
present:
- The model never runs the lint, tests, and/or other checks to verify changes are
working as expected and that no existing functionality is breaking (i.e. regression
checks)
- The model commits a bunch of unnecessary markdown/txt files. These are
usually summaries, explanations of changes, how-to guides, step by step
breakdowns, etc. These files typically don't align with the codebase's
documentation practices, and are often about helping the user in the conversation
rather than helping the user arrive at PR ready code changes.
- The model creates custom impromptu test verification scripts that don't properly
integrate into the codebase's test suites. The conversation ends with those scripts
wrongly included in the final commit
The model creates backup versions of files, but they are left in the final commit.
If you see the model do this, you should steer the model to delete them.

If you spot other common failure patterns across multiple tasks / conversations feel free
to voice them in the discord chat!

Rubric Axes
On each conversation turn you must rate the model responses against a grading rubric.
These aren't exhaustive considerations, but especially if you struggle to identify code
improvements to meet the 2+ turn minimum, you should look at these questions for
inspiration. It's highly unlikely that there aren't some lingering code quality issues.

Which code has better logic and correctness?
. Does the implementation match the intended behavior?
. Are edge cases and error conditions properly handled?
. Is the control flow clear and free of subtle bugs?
. Are there any off-by-one errors, null painter exceptions, or race conditions?
. Is the algorithm/approach correct for the problem being salved?
. Are boundary conditions (empty inputs, maximum values, etc.) handled correctly?

Which code has better naming and clarity?
. Do variable, function, and dlass names clearly express their purpose?
. Is domain terminology used consistently throughout?
. Are boolean names and conditions expressed positively when possible?
. Do names avoid ambiguous abbreviations or insider knowledge?
. Are assumptions about inputs, outputs, or behavior clearly documented?
. Would a new developer understand what each component does from its name alone?
. Are units clear in variable names (e.g., delaySeconds vs delay)?

Which code has better organization and modularity?
. Are functions/methods focused on a single responsibility?
. Is there duplicate code that should be extracted into reusable functions?
. Are source files reasonably sized (not thousands of lines)?
. Are functions/methods concise and focused (not hundreds of lines)?
. Is related functionality grouped together logically?
. Are abstraction levels consistent (not mixing high and low-level operations)?
. Is there proper separation of concerns (I/O separate from business logic)?
. Does each class have high cohesion (all methods relate to its purpose)?
. Is cyclomatic complexity reasonable (avoiding deeply nested code)?
. Are there parallel implementations of the same functionality?




If you spot other common failure patterns across multiple tasks / conversations feel free
to voice them in the discord chat!

Rubric Axes
On each conversation turn you must rate the model responses against a grading rubric.
These aren't exhaustive considerations, but especially if you struggle to identify code
improvements to meet the 2+ turn minimum, you should look at these questions for
inspiration. It's highly unlikely that there aren't some lingering code quality issues.

Which code has better logic and correctness?
. Does the implementation match the intended behavior?
. Are edge cases and error conditions properly handled?
. Is the control flow clear and free of subtle bugs?
. Are there any off-by-one errors, null pointer exceptions, or race conditions?
. Is the algorithm/approach correct for the problem being solved?
. Are boundary conditions (empty inputs, maximum values, etr.) handled correctly?

Which code has better naming and clarity?
. Do variable, function, and dlass names clearly express their purpose?
. Is domain terminology used consistently throughout?
. Are boolean names and conditions expressed positively when possible?
. Do names avoid ambiguous abbreviations or insider knowledge?
. Are assumptions about inputs, outputs, or behavior clearly documented?
. Would a new developer understand what each component does from its name alone?
. Are units clear in variable names e.g, delaySeconds vs delay)?

Which code has better organization and modularity?
. Are functions/methods focused on a single responsibility?
. Is there duplicate code that should be extracted into reusable functions?
. Are source files reasonably sized (not thousands of lines)?
. Are functions/methods concise and focused (not hundreds of lines)?
. Is related functionality grouped together logically?
. Are abstraction levels consistent (not mixing high and low-level operations)?
. Is there proper separation of concerns (I/O separate from business logic)?
. Does each class have high cohesion (all methods relate to its purpose)?
. Is cyclomatic complexity reasonable (avoiding deeply nested code)?
. Are there parallel implementations of the same functionality?

Which code has better interface design?
. Are APls intuitive and hard to misuse?
. Do function signatures minimize coupling (avoiding unnecessary parameters)?
. Are return values and side effects predictable and well-documented?
. Is mutability controlled and explicit?
. Do functions have reasonable parameter counts (s5, using objects for complex configs)?
. Are return types consistent (avoiding different types based on conditions)?
. Is it clear what each function does without reading its implementation?
. Are required vs optional parameters clearly distinguished?
. Do interfaces follow established patterns and conventions?

Which code has better error handling and robustness?
. Are specific exception types used with contextual error messages?
. Is there a consistent error handling strategy (fail fast vs recovery)?
. Is input validation performed early at system boundaries?
. Are errors properly propagated rather than silently swallowed?
. Is resource management handled properly (files closed, memory freed)?
. Are there any bare except clauses that could hide bugs?
. Do error messages provide enough context to debug issues?
. Are partial failures handled gracefully?
. Is defensive programming used appropriately (not excessively)?

Which code has better comments and documentation?
. Do comments explain WHY something is done, not WHAT is being done?
. Are complex algorithms or business logic clearly explained?
. Have comments been updated to match code changes?
. Are there any Al-generated chain-of-thought comments that should be remaved?
. Are there placeholder comments saying code was removed/replaced?
. Is there appropriate documentation for public APls?
. Are edge cases and non-obvious behavior documented?
. Are there too many obvious comments that add noise?
. Do comments provide value to future maintainers?

Which code is more ready for review/merge?
. Is there any debug code, print statements, or console log calls?

. Are commit messages clear and follow project guidelines?
. Is version contral hygiene maintained (no large binary files, etc.)?
. Are all tests passing and coverage adequate?
. Has the code been linted and does it pass static analysis?
. Are there any hardcoded values that should be configurable?
. Is sensitive information (passwords, keys) properly handled?

Prompt Dos and Don'ts

Prompting
To properly prompt the model, you should adhere to the following:
. The prompt should be original. Do not use Al or copy the examples from the
instructions.
Clearly define the scope of your prompt, do not be ambiguous.
. The prompt should align with the project's direction and goals.

DO

Initial Prompt
- The initial prompt should define a clear scope that will be followed
throughout the task.
The initial prompt should align with the project's goals and direction.
- The initial prompt should be realistic and achievable.
Follow-Up Prompts
- Follow-up prompts should steer the direction of the task towards PR
readiness.
Follow-up prompts should stay within the scope of the initial prompt.
Follow-up prompts should encourage writing tests if the request is a feature.
- Follow-up prompts should make sure that existing functionality is not broken
and there are no regressions.

DONTs
Initial Prompt
- The initial prompt should not have many radically different feature requests
that aren't related to each other.
The initial prompt should not be too vague or open-ended.
- The initial prompt should not request the model to use real-time, current
information or data.

The initial prompt should not ask the model to use the Internet. It does not
have access to the internet.
Follow-Up Prompts
- The follow-up prompts should not introduce new features that don't fit
within initial prompt scope.
- The follow-up prompts should not "break the 4th wall" and complain about
platform-related issues. These are not related to the model itself.

Examples of Prompts

Good
cmgh4kukk000207231c447kqq
Repo https://github.com/margbritt/Syglorix/
Turn 1
I would like to extend the CLI command for initializing an empty project. I would like
to provide the ability to initialize the project with different templates. Evaluate the
project and come up with the best approach. I want the template command invoked
via this flag -t/ -- template so the command would look something like 'syqlorix init
my_app -- template minimal' or 'syqlorix init my_app -t api'. I want you to start with
two templates minimal and possibly tailwind .. Look at the demo files and see how
you can leverage those to help create this new functionality. I want you to deliver
something that is PR ready.

Turn 2:
Great start, I noticed that you didn't leverage the demo files to help create the
templates also putting all templates in a single templates.py doesn't seem very
easily maintainable so please come up with a better design for that. The API
template has a questionable approach as well - using DOM manipulation rather
than HTTP response. The minimal template has styles which is not exactly minimal
and the styles should be omitted for it to be truly minimal.

Turn 3
I noticed you removed some of the existing templates from the templates.py file,
please include the api and components templates as well. I also noticed that you
still include both setup.py file and pyproject.toml manifest. Please resolve these
issues

Turn 2:
We want to extend the SDK with support for Cohere. Create a new model class
CohereModel inside src/strands/models/cohere.py. following the same structure as
OpenAlModel and AnthropicModel. The model should support both synchronous
(call) and asynchronous (invoke_async) usage, using Cohere's Chat API.

Notes: The follow up prompt deviates significantly from the original prompt intent
and does not stay within scope.

Commenting/Rating Axis

DOs
- Comments should be written after every turn. (Turn is one model response to
a human prompt)
- Comments should accurately reflect the ratings and vice-versa.
- Ratings should only be given when applicable (use N/A appropriately)
- Make sure the multi-axis rating actually reflects the comparative scores, so
they aren't overly polarized to the point where one model's response looks
completely useless even though both had similar shortcomings.

DONTs
- Comments should not be vague or general, such as "The code was good
overall"
- Comments should not be missing

Examples of Comments

Good

cmgh6iz62ab1z0715nrvtepwr

Model A provides runnable test code that verifies initialization. Model B provides a
summary report claiming everything works. Model A is better because it gives executable
verification that users can run, while Model B only documents claims without providing the
code to prove them. Executable tests beat status reports.

Bad

cmgh4d4i0a9gr0715nj4aw4a5

the two responses are similar up to 80% but most of the time answer b is more specific,
accurate, detailed but makes more space and execution time and sometimes with less
visualization and human readability